{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_model_cat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1J01v3BLSc8HhAOhB0ZUDmXs-NQE3Azp8",
      "authorship_tag": "ABX9TyPWCputdCO0aeNJsEqxdTXP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maybachar/getpet-recognition/blob/master/training_model_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMlTMMVT4dL8"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# num of breeds and image size.\n",
        "num_breeds = 12\n",
        "size = 224\n",
        "\n",
        "\n",
        "def build():\n",
        "    # The input size the model will get. default size for mobile and 3 is for colors.\n",
        "    inputs_model = Input((size, size, 3))\n",
        "    # create the base pre-trained model.\n",
        "    base_model = DenseNet169(input_tensor=inputs_model, include_top=False,weights=\"imagenet\", input_shape=(size, size, 3))\n",
        "    # set all parameters to be trainable\n",
        "    base_model.trainable = True\n",
        "    # get the model so we can add layers to it.\n",
        "    model_out = base_model.output\n",
        "    # add a global spatial average pooling layer\n",
        "    model_out = GlobalAveragePooling2D()(model_out)\n",
        "    # include a dropout layer to minimize the overfitting.\n",
        "    model_out = Dropout(0.2)(model_out)\n",
        "    # using activation function RELU- widely used in CNN.\n",
        "    model_out = Dense(1024, activation=\"relu\")(model_out)\n",
        "    # add softmax layer for getting probabilities on the breeds.\n",
        "    model_out = Dense(num_breeds, activation=\"softmax\")(model_out)\n",
        "    # create the model with it's layers.\n",
        "    model = tf.keras.Model(inputs_model, model_out)\n",
        "    return model\n",
        "\n",
        "\n",
        "def fix_data_element(path, y):\n",
        "    # one hot encoding to lable (all zero except for the place y).\n",
        "    one_hot_y = [0] * num_breeds\n",
        "    one_hot_y[y] = 1\n",
        "    one_hot_y = np.array(one_hot_y)\n",
        "    # Cast to type as defined.\n",
        "    one_hot_y = one_hot_y.astype(np.int32)\n",
        "    path = path.decode()\n",
        "    # loads a color image from the specified file.\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # change the width, height of an image.\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    # normaliztion.\n",
        "    img = img / 255.0\n",
        "    img = img.astype(np.float32)\n",
        "    return img, one_hot_y\n",
        "\n",
        "def wrap_elements(x, y):\n",
        "    # wrap numpy function as an operation in TensorFlow function.\n",
        "    x, y = tf.numpy_function(fix_data_element, [x, y], [tf.float32, tf.int32])\n",
        "    # resize image to the image size mobilenet expect to get.\n",
        "    x.set_shape((size, size, 3))\n",
        "    y.set_shape((num_breeds))\n",
        "    return x, y\n",
        "\n",
        "def fix_dataset(x, y, batch):\n",
        "    # get the slices of x and y into one dataset.\n",
        "    data_slices = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    # execute fix_data function on every element of the Dataset separately.\n",
        "    data_slices = data_slices.map(wrap_elements)\n",
        "    # combines consecutive elements of a dataset object into batches.\n",
        "    data_slices = data_slices.batch(batch)\n",
        "    return data_slices\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # learning rate 0.0001.\n",
        "    lr = 0.0001\n",
        "    batch = 16\n",
        "    epochs = 3\n",
        "    # paths of train data folder and csv file with labels of each pic.\n",
        "    path = \"/content/drive/MyDrive/cat-breed-identification/\"\n",
        "    train_path = os.path.join(path, \"images/images/*\")\n",
        "    labels_path = os.path.join(path, \"cat_labels.csv\")\n",
        "    # import the CSV file.\n",
        "    labels_file = pd.read_csv(labels_path)\n",
        "    # get all unique breeds in file.\n",
        "    breed = labels_file[\"breed\"].unique()\n",
        "    length_breed=len(breed)\n",
        "    breed_num_dic = {}\n",
        "    # create dictionary of breed and number.\n",
        "    for i in range(length_breed):\n",
        "      breed_num_dic[breed[i]] = i\n",
        "    # ids of photos\n",
        "    all_img_ids = glob(train_path)\n",
        "    # list of breed index by order of image id\n",
        "    img_lables = []\n",
        "    # Append to lables list the suitale lable for each image using info from csv file\n",
        "    # and add the number of breed instead the name breed(using dictionary).\n",
        "    for img_id in all_img_ids:\n",
        "        img_id = img_id.split(\"/\")[-1]\n",
        "        img_id = img_id.split(\".\")[0]\n",
        "        # get breed name by image id from labels.csv\n",
        "        breed_name = list(labels_file[labels_file.id == img_id][\"breed\"])[0]\n",
        "        # get the number of this breed and append it to list.\n",
        "        breed_idx = breed_num_dic[breed_name]\n",
        "        img_lables.append(breed_idx)\n",
        "    # work only with N image id and N lables of the breed id which suitable to the images.\n",
        "    all_img_ids = all_img_ids[:2350]\n",
        "    img_lables = img_lables[:2350]\n",
        "    # spliting data to train and validation.\n",
        "    train_x, validation_x = train_test_split(all_img_ids, test_size=0.2, random_state=42)\n",
        "    train_y, validation_y = train_test_split(img_lables, test_size=0.2, random_state=42)\n",
        "    # dataset\n",
        "    trainSet = fix_dataset(train_x, train_y, batch=batch)\n",
        "    validationSet = fix_dataset(validation_x, validation_y, batch=batch)\n",
        "    #create a CNN model with it's layers.\n",
        "    model = build()\n",
        "    #Configures the model for training. optimizer- adam.\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr), metrics=[\"acc\"])\n",
        "    # Train\n",
        "    #set of functions to be applied at training procedure:\n",
        "    # modelCheckPoint saves the model after every epoch.\n",
        "    # Reduce learning rate when a metric has stopped improving.\n",
        "    callbacks = [ModelCheckpoint(\"model.h5\", verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.000001)]\n",
        "    # Trains the model for number of epochs. evaluate the loss at the end of each epoch.\n",
        "    model.fit(trainSet, validation_data=validationSet, epochs=epochs, callbacks=callbacks)\n",
        "    # save the model and it's weights in H5 format to jeson file.\n",
        "    model_json = model.to_json()\n",
        "    with open(\"/content/drive/MyDrive/cat-breed-identification/model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "        model.save_weights(\"/content/drive/MyDrive/cat-breed-identification/model.h5\")\n",
        "        print(\"Saved model to disk\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}